---
name: 3-verify
description: Complete verification and testing with intelligent issue handling, Context7 research, and agent-based fixing
---

# Verify Skill

## Overview

This skill provides complete verification and testing of implementations. It integrates the interactive test execution from the former /3-verify skill with intelligent debugging capabilities. The skill loads test plans generated by /2-code, guides users through manual testing with real-time issue assessment, uses sequential thinking to decide whether to continue testing or fix issues first, performs Context7 research via specialized agents, and spawns autonomous debug agents to execute fixes.

The skill is universal for backend, frontend, and full-stack web/app projects, supporting all major test frameworks and providing intelligent decision-making for issue handling.

**Trigger**: `/3-verify` command in Claude Code after /2-code implementation

## When to Use

This skill activates in the following scenarios:

**Primary use:**
- `/3-verify` command after /2-code generates test plan (02-tests.md)
- When implementations need verification and testing
- When test failures need intelligent handling

**Context indicators:**
- `.workspace/features/{name}/02-tests.md` exists (test plan from /2-code)
- Implementation needs verification
- Issues found during testing need fixes
- Manual and automated tests need coordination

**Works universally with:**
- Backend projects (Laravel, Node.js, Django, Express, Flask, etc.)
- Frontend projects (React, Vue, Svelte, vanilla JS, etc.)
- Full-stack applications
- Games and interactive applications
- Any test framework (pest, phpunit, jest, vitest, pytest, unittest, etc.)

**NOT for:**
- Initial feature planning (use /1-plan)
- Fresh implementation (use /2-code)
- Quality improvements on passing tests (use /4-refine)

## Workflow

The skill operates through six phases. Execute each phase sequentially with intelligent decision points based on sequential thinking.

### FASE 0: FEATURE SELECTION

**Goal:** Let user select which feature to verify from available features.

**Steps:**

1. **Check if feature name provided with command:**

   **If user ran `/3-verify {feature-name}`:**
   - Skip numbered list
   - Use provided feature name directly
   - Validate it exists in `.workspace/features/{feature-name}/02-tests.md`
   - If valid: proceed to step 4 (confirm selection)
   - If invalid: show error, then fall back to numbered list (step 2)

   **If user ran `/3-verify` without arguments:**
   - Proceed to step 2 (show numbered list)

2. **List available features with test plans (only if no feature name provided):**
   - Scan `.workspace/features/` for all folders with `02-tests.md`
   - For each feature, check if `.worktree` file exists and read worktree path
   - Show numbered list with feature status AND worktree info

   ```
   üìã AVAILABLE FEATURES FOR VERIFICATION:

   | # | Feature | Worktree | Test Plan | Parts/Extends/Changes |
   |---|---------|----------|-----------|----------------------|
   | 1 | checkout | ../project--checkout | ‚úì Ready | 3 parts (01-cart, 02-payment, 03-ui) |
   | 2 | user-profile | ../project--user-profile | ‚úì Ready | - |
   | 3 | notifications | (no worktree) | ‚úó No tests | Run /2-code first |

   ```

   **Worktree column logic:**
   - If `.workspace/features/{name}/.worktree` exists ‚Üí show worktree path
   - If no `.worktree` file ‚Üí show "(no worktree)"

   **Use AskUserQuestion tool:**
   - header: "Feature Selectie"
   - question: "Welke feature wil je verifi√´ren?"
   - options:
     - For each feature, create option with label and description
     - label: "Recent (Recommended)", description: "Meest recent gewijzigde feature"
     - label: "Uitleg", description: "Leg deze stap uit"
   - multiSelect: false

3. **Handle user selection:**
   - If number: map to feature name ‚Üí proceed to step 4
   - If "recent": use Glob to find most recent 02-tests.md ‚Üí proceed to step 4
   - If name only: validate exists ‚Üí proceed to step 4
   - If invalid: show error and re-prompt

4. **Auto-detect part/extend/change (always runs after feature selection):**

   **If feature has NO parts/extends/changes:** Skip to step 5

   **If feature HAS parts/extends/changes:**
   - Read `02-tests.md` to find all sections (`## Part: {name}`, `## Extend: {name}` or `## Change: {name}`)
   - Read `03-verify.md` for verification status (create if not exists)
   - For each section, check status in 03-verify.md:
     - If section has `Status: ‚úì VERIFIED` ‚Üí `‚úì Verified`
     - If section has `Status: ‚óã IN PROGRESS` or missing ‚Üí `‚óã Needs verification`
   - **Auto-select** the first section that needs verification

   **Note:** 03-verify.md is the single source of truth for verification status. See "03-verify.md Format" section below for structure.

   ```
   üìã AUTO-DETECTED SECTION

   Feature: checkout
   Sections: 3 total (1 verified, 2 need verification)

   | # | Name | Type | Status |
   |---|------|------|--------|
   | 1 | 01-cart-models | Part | ‚úì Verified |
   | 2 | 02-payment-backend | Part | ‚óã Needs verification ‚Üê NEXT |
   | 3 | 03-checkout-ui | Part | ‚óã Needs verification |

   Auto-selected: 02-payment-backend (part)
   ```

   **If all sections verified:**
   ```
   ‚úÖ ALL SECTIONS VERIFIED

   Feature: checkout
   All parts/extends/changes are already verified.

   Options:
   - Run /4-refine {feature-name} for functional changes
   - Run /5-refactor {feature-name} for code quality improvements
   ```
   ‚Üí EXIT skill gracefully

5. **Confirm selection:**
   ```
   ‚úÖ SELECTED: {feature-name} {part/extend/change if applicable}

   Checking worktree...
   ```

6. **Verify correct worktree (if .worktree file exists):**

   **Skip if:** task_type is EXTEND or CHANGE (these use parent feature's worktree)

   **Steps:**

   a. **Check for .worktree file:**
      ```bash
      cat .workspace/features/{feature-name}/.worktree
      ```
      - If file exists ‚Üí read worktree path
      - If file doesn't exist ‚Üí continue without worktree (legacy mode)

   b. **Compare current directory with worktree path:**
      ```bash
      # Get current directory (absolute path)
      pwd
      ```
      - If current directory matches worktree path ‚Üí continue to FASE 1
      - If current directory does NOT match ‚Üí prompt user to switch

   c. **If NOT in correct worktree:**
      ```
      ‚ö†Ô∏è WRONG WORKTREE

      Feature "{feature-name}" has a dedicated worktree:
      {worktree-path}

      You are currently in:
      {current-directory}
      ```

      Use AskUserQuestion tool:
      - header: "Worktree"
      - question: "Je zit niet in de juiste worktree. Wat wil je doen?"
      - options:
        - label: "Open worktree (Recommended)"
          description: "Open {worktree-path} in nieuw VSCode venster"
        - label: "Toch hier doorgaan"
          description: "Werk in huidige directory (niet aanbevolen)"
        - label: "Annuleren"
          description: "Stop en switch handmatig"
        - label: "Uitleg"
          description: "Leg uit wat worktrees zijn"
      - multiSelect: false

      **If "Open worktree":**
      ```bash
      code "{worktree-path}"
      ```
      Report:
      ```
      üìÇ WORKTREE OPENED

      VSCode venster geopend voor: {worktree-path}

      Switch naar dat venster en run /3-verify {feature-name} opnieuw.
      ```
      ‚Üí EXIT skill (user continues in other window)

      **If "Toch hier doorgaan":**
      Report:
      ```
      ‚ö†Ô∏è Continuing in current directory (worktree ignored)
      ```
      ‚Üí Continue to FASE 1 (with warning logged)

      **If "Annuleren":**
      ‚Üí EXIT skill gracefully

   d. **If IN correct worktree (or no worktree defined):**
      ```
      ‚úÖ WORKTREE VERIFIED

      Working in: {current-directory}
      Feature: {feature-name}

      ‚Üí Loading test plan...
      ```

**Output:**

```text
‚úÖ VERIFICATION STARTED
Feature: {name}
[If part:] Part: {part-name}
[If extend:] Extend: {extend-name}
[If change:] Change: {change-name}
Worktree: {worktree-path} (verified)
Test plan: {path}
```

---

### FASE 0.5: PARSE USER INPUT (Legacy support)

**Goal:** Handle direct input for backwards compatibility.

**Note:** This fase only runs if user provides input directly with command (e.g., `/3-verify checkout`)

**Steps:**

1. **Parse user input:**
   - If "recent" ‚Üí use Glob to find all `**/**/02-tests.md`, sort by modification time descending (newest first), select most recent
   - If feature name only ‚Üí search in `.workspace/features/{name}/02-tests.md`
   - If contains "/" ‚Üí resolve full path
   - If "feature part" pattern ‚Üí locate tests in `.workspace/features/{feature}/02-tests.md` (look for "## Part: {part}" section)
   - For extend/change ‚Üí tests are in `.workspace/features/{parent}/02-tests.md` (look for "## Extend: {name}" or "## Change: {name}" section)
   - Note: Parts are sections within parent files, not separate folders
   - Validate file exists

2. **Handle validation errors:**

   **If file not found:**
   ```
   ‚ùå TEST PLAN NOT FOUND

   Searched: {path or pattern}

   Possible causes:
   - Feature name misspelled
   - /2-code not yet executed for this feature
   - Test plan in different location

   Available test plans:
   {Use Glob to list all 02-tests.md files with timestamps}
   ```

   **Use AskUserQuestion tool:**
   - header: "Test Plan"
   - question: "Test plan niet gevonden. Wat wil je doen?"
   - options:
     - label: "Opnieuw invoeren (Recommended)", description: "Voer een andere feature naam of pad in"
     - label: "Beschikbare tonen", description: "Toon alle beschikbare test plans"
     - label: "Uitleg", description: "Leg uit wat er mis ging"
   - multiSelect: false
   - If "Opnieuw invoeren" ‚Üí Return to step 2 for new input
   - If "Beschikbare tonen" ‚Üí Show Glob results, then return to step 2
   - If "Uitleg" ‚Üí Explain the error, then return to step 2

   **If "recent" but no test plans exist:**
   ```
   ‚ùå NO TEST PLANS FOUND

   There are no 02-tests.md files in .workspace/features/ yet

   Run /2-code first to implement a feature with test plan.
   ```
   Exit skill

   **If user enters invalid input (not feature name, path, or "recent"):**
   ```
   ‚ö†Ô∏è INVALID INPUT

   Expected:
   - Feature name (e.g. "checkout")
   - Path to test plan (e.g. "checkout/02-tests.md")
   - "recent" for most recent
   ```

   **Use AskUserQuestion tool:**
   - header: "Ongeldige Input"
   - question: "De invoer werd niet herkend. Wat wil je doen?"
   - options:
     - label: "Opnieuw invoeren (Recommended)", description: "Voer een geldige feature naam, pad of 'recent' in"
     - label: "Beschikbare features", description: "Toon alle features met test plans"
     - label: "Uitleg", description: "Leg uit welke invoer geldig is"
   - multiSelect: false
   - If "Opnieuw invoeren" ‚Üí Return to step 2 for new input
   - If "Beschikbare features" ‚Üí Show available features list, then return to step 2
   - If "Uitleg" ‚Üí Explain valid input formats, then return to step 2

3. **Confirm test file:**
   ```
   üìã TEST FILE FOUND

   File: .workspace/features/{name}/02-tests.md
   Feature: {Feature Name}
   Generated: {timestamp}
   ```

   **Use AskUserQuestion tool:**
   - header: "Verify"
   - question: "Start verification for this feature?"
   - options:
     - label: "Yes", description: "Begin verification process"
     - label: "No", description: "Cancel and return to selection"
   - multiSelect: false
   - If "Yes" ‚Üí Proceed to FASE 1
   - If "No" ‚Üí Return to FASE 0 step 2

**Output:**
```
‚úÖ VERIFICATION STARTED
Feature: {name}
Test plan: {path}
```

---

### FASE 1: LOAD TEST PLAN & REQUIREMENTS

**Goal:** Load test plan from /2-code AND testable requirements from /1-plan for requirement-based testing.

**Steps:**

1. **Load ALL files in feature folder:**

   Scan target folder and read ALL existing files for full context:

   | File | Purpose | When Present |
   |------|---------|--------------|
   | `01-intent.md` | Requirements with REQ-IDs (REQUIRED) | Always |
   | `01-research.md` | Patterns (REQUIRED) | Always |
   | `01-architecture.md` | Architecture blueprint | If FASE 3.5 ran |
   | `00-overview.md` | Feature documentation | If previously implemented |
   | `02-implementation.md` | Implementation log | If /2-code ran |
   | `02-tests.md` | Test plan (REQUIRED for /3-verify) | If /2-code ran |
   | `04-refine-*.md` | Previous refinement logs | If /4-refine ran |
   | `05-refactor-*.md` | Previous refactor logs | If /5-refactor ran |

   **Why load all files:**
   - Full context of what was built and how
   - Previous test results inform current verification
   - Understand existing refinements/refactors
   - Part/extend/change context in parent files

2. **Parse 01-intent.md for requirements:**
   - Parse `## Testable Requirements` section (feature-level)
   - If part mode: also parse `## Part: {NN}-{name}` section for part-specific requirements
   - If extend/change mode: also parse `## Extend: {name}` or `## Change: {name}` section
   - Extract all REQ-IDs with their:
     - Description
     - Category (core, api, ui, integration, edge_case)
     - Test type (manual, automated_ui, automated_api, automated_unit)
     - Current passes status (initially all false)

3. **Read 02-tests.md file:**
   - Load test plan from confirmed path
   - Parse test structure and sections
   - For part mode: look for `## Part: {NN}-{name}` section
   - For extend/change mode: look for `## Extend: {name}` or `## Change: {name}` section
   - Identify automated vs manual tests
   - Extract visual regression test components

4. **Parse test structure:**
   - Map 02-tests.md tests to REQ-IDs from requirements
   - Count total requirements for progress tracking
   - Note any Chrome Extension test prompts

5. **Check automated test results:**
   - Read automated test section
   - Note if tests passed/failed/not run yet
   - Prepare for manual test execution

6. **Check for existing progress (session resume):**
   - Check if `03-verify.md` exists in feature folder
   - If exists: read requirement status (‚úì, ‚úó, ‚óã, ‚Äî)
   - Count already-verified (‚úì) vs not-yet-verified (‚úó, ‚óã, ‚Äî) requirements

   **If 03-verify.md exists with progress:**
   ```
   üìã EXISTING PROGRESS FOUND

   Already passing: {X} requirements (‚úì)
   Failing: {Y} requirements (‚úó)
   Skipped: {Z} requirements (‚óã)
   Pending: {W} requirements (‚Äî)
   ```

   **Use AskUserQuestion tool:**
   - header: "Progress"
   - question: "Existing verification progress found. How do you want to proceed?"
   - options:
     - label: "Resume", description: "Continue from where you left off (skip passing requirements)"
     - label: "Restart", description: "Reset all to pending and start from beginning"
     - label: "Review", description: "Show current verification status first"
   - multiSelect: false
   - If "Resume" ‚Üí Skip requirements with ‚úì, start at first non-‚úì requirement
   - If "Restart" ‚Üí Reset all requirements to `‚Äî` in 03-verify.md, start from REQ-001
   - If "Review" ‚Üí Display 03-verify.md content, then re-prompt with Resume/Restart options

   **If 03-verify.md doesn't exist:** Continue normally (file created in FASE 2)

7. **Initialize tracking:**
   - Set up requirement counters (passing/failing/skipped)
   - Prepare issue log with requirement links
   - Create attempt tracking if debugging needed

**Output:**
```
‚úÖ TEST PLAN & REQUIREMENTS LOADED

| Field | Value |
|-------|-------|
| **Mode** | [FEATURE / PART / EXTEND / CHANGE] |
| **Feature** | {name} |
| **Part** | {NN}-{name} (if applicable) |
| **Type** | {Extend/Change}: {name} (if applicable) |

**Context loaded:**

| File | Status | Purpose |
|------|--------|---------|
| 01-intent.md | ‚úì | requirements |
| 01-research.md | ‚úì | patterns |
| 02-implementation.md | ‚úì | implementation |
| 02-tests.md | ‚úì | test plan |
| 00-overview.md | ‚úì | feature docs (if exists) |
| 04-refine-*.md | ‚úì | {count} refinements (if exists) |
| 05-refactor-*.md | ‚úì | {count} refactors (if exists) |

**Requirements found:** {count} testable

| Category | Count |
|----------|-------|
| Core | {X} |
| API | {Y} |
| Edge cases | {Z} |
| Already passing | {X} (from previous session) |

**Automated tests:** {status}

‚Üí Starting interactive verification...
```

**IMPORTANT: Immediately proceed to FASE 2 without waiting for user confirmation. Do NOT stop after showing this output.**

**If no requirements found:**
```
‚ùå NO TESTABLE REQUIREMENTS FOUND

01-intent.md does not contain a "## Testable Requirements" section.

This is required for verification. Please run:
1. /1-plan {feature-name} - to generate requirements
2. /2-code {feature-name} - to regenerate tests per requirement
3. /3-verify {feature-name} - to verify against requirements

Cannot proceed without requirements.
```
‚Üí Stop and wait for user to run /1-plan

---

### FASE 2: INTERACTIVE TEST EXECUTION

**Goal:** Execute tests per requirement with manual user input.

**IMPORTANT: Start presenting requirements IMMEDIATELY upon entering this phase. Do NOT wait for user to ask. Present the first requirement test prompt right away, then continue to the next after each user response.**

#### Initialize 03-verify.md

**At start of FASE 2 (if file doesn't exist):**
1. Create `03-verify.md` in feature folder
2. Add Summary section with all counts at 0 (except Pending = total)
3. Add section for each Part/Extend/Change (or single section for feature)
4. List all requirements from 01-intent.md with status `‚Äî` (pending)

**After each requirement test:**
- Update requirement result in 03-verify.md (‚úì, ‚úó, or ‚óã)
- Update Summary counts
- If issue found: add to "Issues Found" table
- Update section status if all requirements in section tested

#### Requirement-Based Testing

**For each requirement (REQ-ID) - present automatically one by one:**

1. **Show requirement test:**
   ```
   üß™ REQUIREMENT {X}/{total}: {REQ-ID}

   Description: {requirement description}
   Category: {core/api/ui/integration/edge_case}
   Test Type: {manual/automated_ui/automated_api/automated_unit}

   Test Steps:
   1. {Step from 02-tests.md mapped to this requirement}
   2. {Step 2}
   3. {Step 3}
   ```

   **Use AskUserQuestion tool:**
   - header: "Test Result"
   - question: "What is the result for {REQ-ID}?"
   - options:
     - label: "Passes", description: "Requirement works as expected"
     - label: "Fails", description: "Requirement has issues (will prompt for details)"
     - label: "Skip", description: "Skip testing this requirement for now"
   - multiSelect: false
   - Response handling documented in step 2 below

2. **Handle user response:**

   **If "Passes":**
   - Update 03-verify.md: set requirement result to `‚úì` with timestamp
   - Update Summary counts (Passing +1, Pending -1)
   ```
   ‚úÖ {REQ-ID} - Passes ‚úì
   {requirement description}
   ```
   Continue to next requirement

   **If "Fails":**
   ```
   Describe why this requirement fails:
   ```

   Wait for issue description, then:

   **SPAWN 3 ASSESSMENT AGENTS for multi-perspective severity analysis:**

   Launch all 3 agents in parallel (single message with 3 Task tool calls):

   ```
   - Task(subagent_type="assess-user-impact", prompt="[issue context]
     Your mission: Assess this issue from the USER perspective.")

   - Task(subagent_type="assess-technical", prompt="[issue context]
     Your mission: Assess this issue from the TECHNICAL perspective.")

   - Task(subagent_type="assess-business", prompt="[issue context]
     Your mission: Assess this issue from the BUSINESS perspective.")
   ```

   **Wait for all 3 agents, then calculate weighted severity:**
   ```
   Weighted Score = (user_score √ó 0.40) + (technical_score √ó 0.30) + (business_score √ó 0.30)
   ```

   | Score Range | Severity |
   |-------------|----------|
   | 75-100 | CRITICAL |
   | 50-74 | IMPORTANT |
   | 0-49 | SUGGESTION |

   **Show synthesized assessment:**
   ```
   üìä ISSUE ASSESSMENT (Multi-Perspective)

   Agents completed:
   ‚úì assess-user-impact: {X}/100 ({summary})
   ‚úì assess-technical: {Y}/100 ({summary})
   ‚úì assess-business: {Z}/100 ({summary})

   Weighted Score: {W}/100
   Severity: [CRITICAL/IMPORTANT/SUGGESTION]

   Key factors:
   - User: {main user impact}
   - Technical: {root cause}
   - Business: {business risk}

   Recommendation: {continue testing / stop for fix}
   ```

   - Update 03-verify.md: set requirement result to `‚úó` with timestamp and issue description
   - Update Summary counts (Failing +1, Pending -1)
   - Add issue to "Issues Found" table in 03-verify.md
   ```
   ‚ùå {REQ-ID} - Fails ‚úó
   {requirement description}

   Issue: {user description}
   Severity: [CRITICAL] / [IMPORTANT] / [SUGGESTION]
   Location: {file:line}
   ```

   **If CRITICAL:** Stop tests, go to FASE 3
   **If IMPORTANT/SUGGESTION:** Log issue, continue to next requirement

   **If "Skip":**
   - Update 03-verify.md: set requirement result to `‚óã` with "Skipped" note
   - Update Summary counts (Skipped +1, Pending -1)
   - **Check for dependent requirements:**
     - Parse 01-intent.md for requirements that depend on this one
     - **Dependency format in 01-intent.md:**
       ```yaml
       - id: REQ-003
         description: User can remove items from cart
         depends_on: [REQ-001, REQ-002]  # Optional field
         category: core
         test_type: manual
       ```
       **Note:** Verification status (‚úì/‚úó/‚óã/‚Äî) is tracked in 03-verify.md, NOT in 01-intent.md
     - Check `depends_on` field of ALL requirements to find which ones reference the skipped REQ-ID
     - If dependencies exist, warn user before allowing skip:

     ```
     ‚ö†Ô∏è DEPENDENCY WARNING

     {REQ-ID} is being skipped.

     The following requirements depend on this one:
     - {REQ-XXX}: {description} (depends on {REQ-ID})
     - {REQ-YYY}: {description} (depends on {REQ-ID})

     Skipping may cause these tests to fail incorrectly.
     ```

     **Use AskUserQuestion tool:**
     - header: "Skip Deps"
     - question: "This requirement has dependencies. Continue with skip?"
     - options:
       - label: "Yes", description: "Skip this requirement and its dependencies"
       - label: "No", description: "Go back and test dependencies first"
     - multiSelect: false
     - If "Yes": Mark as skipped, continue
     - If "No": Return to requirement test prompt

   - If no dependencies or user confirms:
   ```
   ‚äò {REQ-ID} - Skipped (not verified)
   ```
   Continue to next requirement

3. **Update 01-intent.md after each requirement:**
   - Update the requirement's `passes` status in the file
   - This maintains persistent state across sessions

---

#### Severity Categories

| Level | Criteria | Action |
|-------|----------|--------|
| CRITICAL | App breaks, errors on screen, data loss, blocks functionality | Stop tests, fix first |
| IMPORTANT | Feature doesn't work correctly, wrong output, bad UX | Log, continue tests, fix after |
| SUGGESTION | Styling issues, minor UX, alignment, nice-to-have | Log, optional fix |

---

#### Progress Tracking

```
Progress: 5/10 requirements tested
Passing: 4 | Failing: 1 | Skipped: 0 | Remaining: 5
```

---

**Example requirement test flow:**
```
üß™ REQUIREMENT 3/10: REQ-003

Description: User can remove items from cart
Category: core
Test Type: manual

Test Steps:
1. Add item to cart
2. Click remove button on item
3. Verify item is removed from cart

[AskUserQuestion: header="Test Result", question="What is the result for REQ-003?"]
[Options: Passes | Fails | Skip]

User selects: Passes

‚úÖ REQ-003 - Passes ‚úì
User can remove items from cart
```

---

### FASE 3: OVERALL ISSUE ASSESSMENT

**Goal:** Analyze requirement verification results and determine next steps based on passes/fails status.

**Steps:**

1. **Compile requirement results:**
   - Count requirements by status: passing, failing, skipped
   - Group failing requirements by issue severity
   - Calculate completion percentage: `(passing / total) * 100`

2. **Generate structured results report:**

   **If ALL REQUIREMENTS PASS:**
   ```
   üéâ ALL REQUIREMENTS PASS!

   Requirements: {X}/{X} passing (100%)

   ‚úÖ Passing Requirements:
   - REQ-001: {description} ‚úì
   - REQ-002: {description} ‚úì
   - REQ-003: {description} ‚úì
   [...]

   ## Strengths
   - {Positive observation 1}
   - {Positive observation 2}

   ‚Üí All requirements verified. Ready for commit or /4-refine
   ```
   End workflow successfully

   **If REQUIREMENTS FAIL:**
   ```
   üìä VERIFICATION RESULTS

   Requirements: {X}/{Y} passing ({Z}%)

   ‚úÖ Passing:
   - REQ-001: {description} ‚úì
   - REQ-002: {description} ‚úì

   ‚ùå Not Passing:
   - REQ-003: {description} (CRITICAL issue)
   - REQ-004: {description} (not yet tested)

   ## ‚ùå Critical Issues ({count} found)
   | REQ-ID | Requirement | Issue | Location |
   |--------|-------------|-------|----------|
   | REQ-003 | {description} | {issue} | [{file:line}] |

   ## ‚ö†Ô∏è Important Issues ({count} found)
   | REQ-ID | Requirement | Issue | Location |
   |--------|-------------|-------|----------|
   | REQ-005 | {description} | {issue} | [{file:line}] |

   ## üí° Suggestions ({count} found)
   | REQ-ID | Requirement | Issue | Location |
   |--------|-------------|-------|----------|
   | REQ-007 | {description} | {issue} | [{file:line}] |

   ## Strengths
   - {What's working well}
   ```

3. **Requirement Completion Check:**

   ```python
   def can_complete_verification(requirements, issues):
       total = len(requirements)
       passing = sum(1 for r in requirements if r["passes"])
       failing = total - passing

       critical_issues = sum(
           1 for i in issues
           if i["severity"] == "critical" and i["status"] == "open"
       )

       if failing > 0:
           return False, f"{failing}/{total} requirements not passing"
       if critical_issues > 0:
           return False, f"{critical_issues} critical issues still open"

       return True, "All requirements pass"
   ```

4. **Determine path forward:**

   **If NOT all requirements pass:**
   ```
   ‚ö†Ô∏è CANNOT MARK COMPLETE

   Blocking:
   - {X} requirements not passing
   - {Y} critical issues open
   ```

   **Use AskUserQuestion tool:**
   - header: "Next Step"
   - question: "Requirements are not passing. How do you want to proceed?"
   - options:
     - label: "Fix issues", description: "Go to Context7 research for solutions"
     - label: "Exit", description: "Stop verification, fix manually (state saved for resume)"
   - multiSelect: false
   - If "Fix issues" ‚Üí Continue to FASE 4
   - If "Exit" ‚Üí Exit skill with current state documented in 01-intent.md (can resume later with /3-verify)

   **If all requirements pass:**
   ```
   üéâ VERIFICATION COMPLETE

   All {X} requirements pass.
   ‚Üí Ready for commit or /4-refine
   ```
   End workflow (skip FASE 4-6)

5. **Prepare for debugging if needed:**
   - Create debug folder if not exists
   - Initialize attempt tracking
   - Prepare Context7 research queries based on failing requirements

**Output:**
```
üìä VERIFICATION RESULTS

Requirements: {X}/{Y} passing ({Z}%)

‚úÖ Passing:
- REQ-001: {description} ‚úì
- REQ-002: {description} ‚úì

‚ùå Not Passing:
- REQ-003: {description} (CRITICAL)
- REQ-004: {description} (IMPORTANT)

## Critical Issues ({count} found)
| REQ-ID | Requirement | Issue | Location |
|--------|-------------|-------|----------|
| REQ-003 | {description} | {issue} | [{file:line}] |

## Important Issues ({count} found)
| REQ-ID | Requirement | Issue | Location |
|--------|-------------|-------|----------|
| REQ-004 | {description} | {issue} | [{file:line}] |

## Strengths
- {positive observations}

‚Üí Proceeding to Context7 research for {count} failing requirements...
```

---

### FASE 4: CONTEXT7 RESEARCH VIA AGENTS

**Goal:** Research solutions for found issues using Context7 via specialized agents based on issue type.

**IMPORTANT:** Use sequential thinking throughout this phase.

**Steps:**

1. **Use sequential thinking to plan research:**
   ```
   [Sequential thinking]
   - Analyzing {count} issues by category
   - CRITICAL issues: {list}
   - IMPORTANT issues: {list}
   - Mapping issues to specialized researchers
   - Planning parallel agent spawning
   ```

2. **Spawn specialized research agents based on issue type:**

   **Match issue type to specialized agent:**
   | Issue Type | Agent | Focus |
   |------------|-------|-------|
   | Errors, exceptions, crashes | error-handling-researcher | Try/catch, error boundaries, exception handling |
   | Performance, slow, memory | performance-researcher | N+1 queries, caching, optimization |
   | Security, auth, validation | security-researcher | Input validation, auth flows, OWASP |
   | Code quality, patterns | quality-researcher | Design patterns, SOLID, maintainability |
   | Type errors, UI bugs, general | best-practices-researcher | Framework patterns, conventions, idioms |

   **Note:** For issues not matching specific agents (TypeScript types, UI/styling, general bugs), use `best-practices-researcher` with a focused prompt describing the specific issue type.

   ```python
   # Spawn parallel specialized agents
   # Example: Database error ‚Üí error-handling-researcher
   Task(
       description="Research error handling patterns",
       prompt="""
       Research solutions for this CRITICAL error:
       Issue: Database connection error
       Location: src/auth/login.ts:23

       Use Context7 to find:
       - Error handling patterns for database connections
       - Retry logic and circuit breaker patterns
       - Graceful degradation approaches
       """,
       subagent_type="error-handling-researcher"
   )

   # Example: TypeScript error ‚Üí best-practices-researcher (with type focus)
   Task(
       description="Research type issue fix",
       prompt="""
       Research solutions for this IMPORTANT type issue:
       Issue: Type 'string' is not assignable to type 'number'
       Location: src/cart/total.ts:89

       Focus on TypeScript best practices:
       - Proper type definitions
       - Type conversion patterns
       - Generic type solutions
       """,
       subagent_type="best-practices-researcher"
   )

   # Example: UI bug ‚Üí best-practices-researcher (with UI focus)
   Task(
       description="Research UI rendering issue",
       prompt="""
       Research solutions for this SUGGESTION:
       Issue: Logo alignment off on mobile
       Location: src/components/Header.tsx:12

       Focus on UI/CSS best practices:
       - Responsive design patterns
       - Flexbox/Grid alignment solutions
       - Mobile-first approaches
       """,
       subagent_type="best-practices-researcher"
   )
   ```

3. **Collect and synthesize findings:**
   - Wait for all agents to return
   - Compile Context7 results per issue category
   - Assess solution quality (target ‚â•75% relevance)
   - Identify common patterns across issues

4. **Spawn fix synthesizer agent with quality metadata:**
   ```python
   Task(
       description="Synthesize fix strategies",
       prompt="""
       Analyze these Context7 findings and create prioritized fix strategies:

       CRITICAL findings: {critical_results}
       - Average relevance: {critical_avg_relevance}%
       - Quality flag: {"‚ö†Ô∏è LOW QUALITY" if < 75% else "‚úì Good quality"}

       IMPORTANT findings: {important_results}
       - Average relevance: {important_avg_relevance}%
       - Quality flag: {"‚ö†Ô∏è LOW QUALITY" if < 75% else "‚úì Good quality"}

       SUGGESTION findings: {suggestion_results}
       - Average relevance: {suggestion_avg_relevance}%
       - Quality flag: {"‚ö†Ô∏è LOW QUALITY" if < 75% else "‚úì Good quality"}

       Create actionable fix plan ordered by priority.

       IMPORTANT: For any fix with relevance < 75%, mark it as:
       "‚ö†Ô∏è EXPERIMENTAL - may need manual adjustment"

       Include confidence percentage for each fix step.
       """,
       subagent_type="fix-synthesizer"
   )
   ```

   **Quality propagation rules:**
   - Fixes with ‚â•75% relevance: Normal confidence
   - Fixes with 50-74% relevance: Mark as "may need adaptation"
   - Fixes with <50% relevance: Mark as "‚ö†Ô∏è EXPERIMENTAL"

**Output:**
```
üîç CONTEXT7 RESEARCH COMPLETE

**Specialized agents used:**

| Agent | Issues |
|-------|--------|
| error-handling-researcher | {count} |
| performance-researcher | {count} |
| security-researcher | {count} |
| quality-researcher | {count} |
| best-practices-researcher | {count} |

**Results by priority:**

## Critical Issues

| Issue | Solution | Relevance | Confidence |
|-------|----------|-----------|------------|
| {Issue} | {Solution found} | {X}% | {Y}% |

## Important Issues

| Issue | Solution | Relevance | Confidence |
|-------|----------|-----------|------------|
| {Issue} | {Solution found} | {X}% | {Y}% |

## Suggestions

| Issue | Solution | Relevance | Confidence |
|-------|----------|-----------|------------|
| {Issue} | {Solution found} | {X}% | {Y}% |

| Metric | Value |
|--------|-------|
| **Average relevance** | {X}% |
| **Average confidence** | {Y}% |

‚Üí Creating debug plan...
```

**Send notification (after FASE 4 research agents):**
```bash
powershell -ExecutionPolicy Bypass -File .claude/scripts/notify.ps1 -Title "Claude Code" -Message "Debug plan ready"
```

---

### FASE 5: CREATE & EXECUTE DEBUG PLAN (with Parallel Fix Strategy Agents)

**Goal:** Generate multiple fix strategies using parallel agents, let user choose approach, then spawn autonomous debug agent.

**IMPORTANT:** This phase uses 3 parallel fix strategy agents to provide different approaches for user choice.

**Steps:**

1. **Prepare fix context from Context7 research:**

   Compile the following for all fix strategy agents:
   ```
   Issues to fix:

   Issue 1: {description}
   - REQ-ID: {REQ-XXX}
   - Severity: {CRITICAL/IMPORTANT/SUGGESTION}
   - Location: {file:line}
   - Context7 research findings: {relevant solutions}

   [... more issues ...]

   Affected files: {file list}
   ```

2. **Launch 3 fix strategy agents in parallel (single message with 3 Task tool calls):**

   ```
   üîß Generating fix strategies...

   3 fix strategy agents proposing approaches:
   - Agent 1: Minimal ("Smallest change that works")
   - Agent 2: Thorough ("Fix it right, fix it once")
   - Agent 3: Defensive ("Never let this happen again")
   ```

   ```
   - Task(subagent_type="fix-minimal", prompt="[context above]
     Your mission: Create a MINIMAL fix plan (smallest change, lowest risk).")

   - Task(subagent_type="fix-thorough", prompt="[context above]
     Your mission: Create a THOROUGH fix plan (complete solution, root cause addressed).")

   - Task(subagent_type="fix-defensive", prompt="[context above]
     Your mission: Create a DEFENSIVE fix plan (add safeguards, prevent recurrence).")
   ```

3. **Wait for all 3 agents, then present fix strategies:**

   **If `manual_solution.provided == True` (user provided solution in FASE 4):**
   ```
   üìã 4 FIX STRATEGIES READY (includes your solution)

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ üìä FIX STRATEGIES COMPARISON                                               ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                                                             ‚îÇ
   ‚îÇ  Strategy        ‚îÇ Changes ‚îÇ Risk    ‚îÇ Time      ‚îÇ Best For                ‚îÇ
   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
   ‚îÇ 1. MINIMAL       ‚îÇ ~[N]    ‚îÇ Low     ‚îÇ [X] min   ‚îÇ Quick hotfix, urgent    ‚îÇ
   ‚îÇ 2. THOROUGH      ‚îÇ ~[N]    ‚îÇ Medium  ‚îÇ [X] hr    ‚îÇ Root cause, no repeat   ‚îÇ
   ‚îÇ 3. DEFENSIVE     ‚îÇ ~[N]    ‚îÇ Medium  ‚îÇ [X] hr    ‚îÇ Prevent future issues   ‚îÇ
   ‚îÇ 4. YOUR SOLUTION ‚îÇ ~[N]    ‚îÇ Unknown ‚îÇ [X] min   ‚îÇ Your proposed approach  ‚îÇ
   ‚îÇ    "{summary}"   ‚îÇ lines   ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îÇ                  ‚îÇ         ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ```

   **Use AskUserQuestion tool:**
   - header: "Fix Strategie"
   - question: "Welke fix strategie wil je gebruiken?"
   - options:
     - label: "Minimal (Recommended)", description: "Snelle fix, laagste risico - kleinste wijziging"
     - label: "Thorough", description: "Root cause aanpak, complete oplossing"
     - label: "Defensive", description: "Voeg safeguards toe, voorkom herhaling"
     - label: "Jouw oplossing", description: "Gebruik je eigen voorgestelde aanpak uit FASE 4"
     - label: "Bekijk details", description: "Toon volledige strategie details"
     - label: "Annuleren", description: "Stop en fix handmatig"
     - label: "Uitleg", description: "Leg de verschillen uit"
   - multiSelect: false

   **If no manual_solution (standard flow):**
   ```
   üìã 3 FIX STRATEGIES READY

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ üìä FIX STRATEGIES COMPARISON                                               ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                                                             ‚îÇ
   ‚îÇ  Strategy        ‚îÇ Changes ‚îÇ Risk    ‚îÇ Time      ‚îÇ Best For                ‚îÇ
   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
   ‚îÇ 1. MINIMAL       ‚îÇ ~[N]    ‚îÇ Low     ‚îÇ [X] min   ‚îÇ Quick hotfix, urgent    ‚îÇ
   ‚îÇ    "Smallest"    ‚îÇ lines   ‚îÇ         ‚îÇ           ‚îÇ production fix          ‚îÇ
   ‚îÇ                  ‚îÇ         ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îÇ 2. THOROUGH      ‚îÇ ~[N]    ‚îÇ Medium  ‚îÇ [X] hr    ‚îÇ Root cause, no repeat   ‚îÇ
   ‚îÇ    "Complete"    ‚îÇ lines   ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îÇ                  ‚îÇ         ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îÇ 3. DEFENSIVE     ‚îÇ ~[N]    ‚îÇ Medium  ‚îÇ [X] hr    ‚îÇ Prevent future issues   ‚îÇ
   ‚îÇ    "Safeguards"  ‚îÇ lines   ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îÇ                  ‚îÇ         ‚îÇ         ‚îÇ           ‚îÇ                         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ```

   **Use AskUserQuestion tool:**
   - header: "Fix Strategie"
   - question: "Welke fix strategie wil je gebruiken?"
   - options:
     - label: "Minimal (Recommended)", description: "Snelle fix, laagste risico - kleinste wijziging"
     - label: "Thorough", description: "Root cause aanpak, complete oplossing"
     - label: "Defensive", description: "Voeg safeguards toe, voorkom herhaling"
     - label: "Bekijk details", description: "Toon volledige strategie details"
     - label: "Annuleren", description: "Stop en fix handmatig"
     - label: "Uitleg", description: "Leg de verschillen uit"
   - multiSelect: false

4. **Process user choice:**

   - **If "Minimal"** ‚Üí use fix-minimal output, proceed to step 5
   - **If "Thorough"** ‚Üí use fix-thorough output, proceed to step 5
   - **If "Defensive"** ‚Üí use fix-defensive output, proceed to step 5
   - **If "Jouw oplossing"** ‚Üí use manual_solution.approach, proceed to step 5 (only if manual_solution provided)
   - **If "Bekijk details"** ‚Üí show selected strategy details, return to choice
   - **If "Annuleren"** ‚Üí exit skill with state documented

5. **Show selected fix plan and confirm:**

   ```
   üìã SELECTED FIX STRATEGY: [MINIMAL/THOROUGH/DEFENSIVE]

   Based on research and analysis:

   1. [CRITICAL] Fix {critical issue} in {file}
      - REQ-ID: {REQ-XXX}
      - Reason: {why}
      - Solution: {specific approach from Context7}
      - Confidence: {X}% {‚ö†Ô∏è LOW CONFIDENCE if < 75%}

   2. [IMPORTANT] Update {component} to handle {error}
      - REQ-ID: {REQ-XXX}
      - Change: {specific modification}
      - Impact: {what this fixes}
      - Confidence: {X}%

   [... more fixes based on selected strategy ...]

   Summary:
   - Critical fixes: {X} (must succeed)
   - Important fixes: {Y} (should succeed)
   - Suggestions: {Z} (optional)

   Estimated time: ~{X} minutes
   Risk level: {Low/Medium/High}

   ```

   **Use AskUserQuestion tool:**
   - header: "Uitvoeren"
   - question: "Dit fix plan uitvoeren?"
   - options:
     - label: "Ja (Recommended)", description: "Voer het plan uit zoals getoond"
     - label: "Nee", description: "Annuleer en probeer andere aanpak"
     - label: "Aanpassen", description: "Wijzig specifieke stappen voor uitvoering"
     - label: "Uitleg", description: "Leg uit wat er gaat gebeuren"
   - multiSelect: false

   **Note:** Always preserve `[CRITICAL]`, `[IMPORTANT]`, or `[SUGGESTION]` prefix for each fix step to maintain severity context throughout the workflow.

6. **Handle user response:**

   **If "Ja":**
   ```python
   # Spawn autonomous debug executor with selected strategy
   result = Task(
       description="Execute debug plan autonomously",
       prompt=f"""
       Execute the following {strategy_name} fix plan step by step:

       {selected_fix_plan}

       Apply all fixes, test after each change, report results.
       """,
       subagent_type="debug-executor"
   )
   ```

   **If "Nee":**
   ```
   Fix plan geannuleerd.
   ```

   **Use AskUserQuestion tool:**
   - header: "Geannuleerd"
   - question: "Fix plan is geannuleerd. Wat wil je doen?"
   - options:
     - label: "Andere strategie (Recommended)", description: "Terug naar strategie selectie"
     - label: "Handmatig", description: "Stop skill, fix zelf de issues"
     - label: "Overslaan", description: "Ga verder naar rapport (issues blijven open)"
     - label: "Uitleg", description: "Leg de opties uit"
   - multiSelect: false
   - If "Andere strategie" ‚Üí Return to step 3 (strategy selection)
   - If "Handmatig" ‚Üí Exit skill with current state documented
   - If "Overslaan" ‚Üí Skip to FASE 6 (generate report with unfixed issues)

   **If "Aanpassen":**

   **Use AskUserQuestion tool:**
   - header: "Stap Aanpassen"
   - question: "Welke stap wil je aanpassen?"
   - options:
     - label: "Stap 1", description: "{Korte beschrijving stap 1}"
     - label: "Stap 2", description: "{Korte beschrijving stap 2}"
     - label: "Stap {N}", description: "{Korte beschrijving stap N}"
     - label: "Annuleren", description: "Terug naar fix plan"
     - label: "Uitleg", description: "Leg uit wat elke stap doet"
   - multiSelect: false
   - Note: Generate options dynamically based on actual fix plan steps

   After step selection, show:
   ```
   Current step {N}:
   {Show current step content}

   Wat wil je wijzigen?
   ```
   Wait for modification description, then:
   - Update step {N} in fix plan with user's changes
   - Show updated full plan:
   ```
   üìã AANGEPAST FIX PLAN

   {Updated plan with modified step highlighted}
   ```

   **Use AskUserQuestion tool:**
   - header: "Uitvoeren"
   - question: "Dit aangepaste plan uitvoeren?"
   - options:
     - label: "Ja (Recommended)", description: "Voer het aangepaste plan uit"
     - label: "Nee", description: "Annuleer en probeer andere aanpak"
     - label: "Aanpassen", description: "Wijzig nog een stap"
     - label: "Uitleg", description: "Leg de wijzigingen uit"
   - multiSelect: false

   Return to step 6 (re-prompt for approval)

7. **Monitor debug execution:**
   - Debug-executor agent works autonomously
   - Reports progress updates
   - Returns when complete or if blocked

8. **Process debug results:**
   ```
   ‚úÖ DEBUG EXECUTION COMPLETE

   **Fixed issues:** {count}

   | Severity | Fixed |
   |----------|-------|
   | CRITICAL | {X} |
   | IMPORTANT | {Y} |
   | SUGGESTION | {Z} |

   **Files modified:**

   | File | Lines | Severity | REQ-ID | Description |
   |------|-------|----------|--------|-------------|
   | {file1} | {X}-{Y} | CRITICAL | {REQ-XXX} | {brief description} |
   | {file2} | {X} | IMPORTANT | {REQ-XXX} | {brief description} |

   **Changes applied:**

   | Severity | Change | Status |
   |----------|--------|--------|
   | CRITICAL | {Change 1} | ‚úì |
   | IMPORTANT | {Change 2} | ‚úì |
   | SUGGESTION | {Change 3} | ‚úì |

   ‚Üí Ready to re-verify
   ```

**Output:**
```
‚úÖ FIXES APPLIED

All debug steps executed successfully.
Ready for re-verification.

‚Üí Returning to test execution...
```

---

### FASE 6: FINAL VERIFICATION

**Send notification (after FASE 5 debug execution):**
```bash
powershell -ExecutionPolicy Bypass -File .claude/scripts/notify.ps1 -Title "Claude Code" -Message "Fixes applied"
```

**Goal:** Re-run tests to verify all fixes work correctly.

**Steps:**

1. **Re-run automated tests:**
   ```bash
   # Execute via script
   python scripts/run_tests.py
   ```

2. **Re-execute failed manual tests:**
   - If testing was stopped by CRITICAL: Re-test the CRITICAL component first, then continue with remaining untested components
   - If testing completed normally: Only re-test components that failed
   - Use same interactive flow as FASE 2
   - Verify fixes resolved issues
   - **If CRITICAL component still fails on re-test:** Stop remaining tests immediately, add to "issues remain" list, proceed to step 3 with failure outcome

   **Extension/Change feature handling:**
   - If verifying extend/change (detected by "## Extend:" or "## Change:" sections in 01-intent.md):
     - Tests are in the parent feature's 02-tests.md file
     - Look for the specific extend/change section within the test file

     ```
     üìã EXTENSION DETECTED

     Parent feature: {parent}
     Extension type: Extend/Change
     Extension name: {name from section header}
     ```

     **Use AskUserQuestion tool:**
     - header: "Parent Impact"
     - question: "Does this extension modify parent feature behavior?"
     - options:
       - label: "Yes", description: "Changes affect the parent feature"
       - label: "No", description: "Extension is isolated from parent"
     - multiSelect: false

     - If "Yes":
       - Identify affected parent requirements
       - Add parent's affected requirements to re-verification queue
       - After extension verification, update parent's 01-intent.md if requirements changed
       - Document extension impact in parent's 00-overview.md History section

     - If "No":
       - Only verify extension-specific requirements
       - No parent updates needed

3. **Evaluate re-verification results:**

   **If ALL tests now pass:**
   ```
   ‚úÖ VERIFICATION COMPLETE

   Initial results:
   - Failed: {X} issues

   After fixes:
   - All issues resolved ‚úì

   Final status: VERIFIED

   Test file updated: {path}/02-tests.md

   Feature is ready for:
   - Commit changes
   - Or run /4-refine for quality improvements
   ```

   End workflow successfully

   **If issues REMAIN (fixes didn't work):**
   ```
   ‚ö†Ô∏è ISSUES NOT RESOLVED

   Original issues: {X}
   Still open: {Y}

   Remaining issues:
   | Component | Issue | Status |
   |-----------|-------|--------|
   | {Component} | {Issue} | Still failing |
   ```

   **Use AskUserQuestion tool:**
   - header: "Unresolved"
   - question: "Issues were not resolved. How do you want to proceed?"
   - options:
     - label: "Retry", description: "Back to Context7 research (FASE 4)"
     - label: "Manual", description: "Exit skill, fix yourself"
     - label: "Report", description: "Generate report with open issues"
   - multiSelect: false
   - If "Retry" ‚Üí Return to FASE 4 with remaining issues (increment attempt counter)
   - If "Manual" ‚Üí Exit skill with current state documented
   - If "Report" ‚Üí Generate partial success report, end workflow

   **If NEW issues discovered (fix introduced regression):**
   ```
   ‚ö†Ô∏è NEW ISSUES FOUND

   Original issues: {X} (fixed ‚úì)
   New issues: {Y}

   New issues:
   | Component | Issue | Severity |
   |-----------|-------|----------|
   | {Component} | {New issue} | {CRITICAL/IMPORTANT/SUGGESTION} |

   This may be a regression from the fixes.
   ```

   **Use AskUserQuestion tool:**
   - header: "Regression"
   - question: "New issues found (possible regression). How do you want to proceed?"
   - options:
     - label: "Analyze", description: "Back to FASE 3 for new issues"
     - label: "Rollback", description: "Undo fixes (if possible)"
     - label: "Report", description: "Generate report with new issues"
   - multiSelect: false
   - If "Analyze" ‚Üí Return to FASE 3 with new issues (start new debug cycle)
   - If "Rollback" ‚Üí Attempt to rollback changes, re-verify
   - If "Report" ‚Üí Generate report documenting regression, end workflow

   **State Transition Rules for Regression (Option 1):**
   When returning to FASE 3 after regression detection:
   ```python
   # State preservation rules
   for req in requirements:
       if req.id in original_failing_reqs and req.passes == True:
           # Originally failed, now fixed - KEEP passes=true
           req.fixed_in_cycle = current_cycle
       elif req.id in new_issue_reqs:
           # New regression - SET passes=false
           req.passes = False
           req.regression_from = current_cycle
       else:
           # Unaffected requirements - PRESERVE current state
           pass

   # Increment cycle counter
   attempt_counter += 1

   # Check max attempts (prevent infinite loop)
   if attempt_counter >= 3:
       # Force exit with report
       show_message("Maximum debug cycles (3) reached. Generating report...")
       goto FASE_6_REPORT
   ```

   **Re-test scope in new cycle:**
   - Test ONLY: new regression issues + previously-fixed issues (regression check)
   - Do NOT re-test: requirements that were already passing before debug cycle

4. **Update test results:**
   - Update 02-tests.md with final results
   - Add timestamp with time:get_current_time
   - Document resolution of issues (or remaining issues)
   - Record attempt count if multiple iterations

5. **Update feature overview (00-overview.md):**
   - Read `.workspace/features/{name}/00-overview.md` (if exists)
   - Update the following sections:

   **A. Update Status table:**
   ```markdown
   ## Status

   | Pipeline | Requirements | Updated |
   |----------|--------------|---------|
   | `VERIFIED` | {X}/{Y} | {date} |
   ```

   **B. Append to History:**
   ```markdown
   ## History

   | Date | Phase | Summary |
   |------|-------|---------|
   | {date} | Verify | {X}/{Y} requirements |
   ```

6. **Auto-commit changes:**
   ```bash
   git add .
   git commit -m "$(cat <<'EOF'
verify({name}): {summary}

{description}
EOF
)"
   ```

   **Commit message format:**
   - `{name}`: Feature name
   - `{summary}`: One-line summary (e.g., "Verify checkout feature - all tests pass")
   - `{description}`: 2-3 lines describing verification results:
     - Requirements passing count (X/Y)
     - Issues fixed (if any)
     - Final status (VERIFIED or issues remain)

   **IMPORTANT:** Do NOT add Co-Authored-By, ü§ñ Generated with Claude Code, or any other footer to pipeline commits.

7. **Show copyable next commands:**

   After verification completion, always display two options:

   ```text
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   üìã NEXT COMMANDS (copy after /clear):

   Option 1 - Small functional changes:
   /4-refine {feature-name}

   Option 2 - Code quality improvements:
   /5-refactor {feature-name}

   Option 3 - All done (no further changes needed)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

   üí° Use /4-refine for: adding small features, tweaking behavior, UX adjustments
   üí° Use /5-refactor for: code cleanup, performance, security, maintainability
   üí° Skip both if: all tests pass and no improvements needed
   ```

---

## 03-verify.md Format

The `03-verify.md` file is the **single source of truth** for verification status. It is created at the start of FASE 2 and updated after each requirement test.

**Location:** `.workspace/features/{feature-name}/03-verify.md`

### File Structure

```markdown
# Verification: {feature-name}

## Summary
| Status | Count |
|--------|-------|
| ‚úì Passing | {X} |
| ‚úó Failing | {Y} |
| ‚óã Skipped | {Z} |
| ‚Äî Pending | {W} |
| **Total** | **{N}** |

Progress: {X}% | Last updated: {timestamp}

---

## Part: {part-name}
**Status: ‚úì VERIFIED** | **‚óã IN PROGRESS** | **‚Äî NOT STARTED**

| REQ | Description | Result | When | Issue |
|-----|-------------|--------|------|-------|
| REQ-001 | {description} | ‚úì | {time} | |
| REQ-002 | {description} | ‚úó | {time} | {issue description} |
| REQ-003 | {description} | ‚óã | {time} | Skipped: {reason} |
| REQ-004 | {description} | ‚Äî | ‚Äî | |

### Issues Found
| REQ | Severity | Issue | Location |
|-----|----------|-------|----------|
| REQ-002 | CRITICAL | {issue} | {file:line} |

---

## Part: {next-part}
...
```

### Status Symbols
| Symbol | Meaning | Used For |
|--------|---------|----------|
| ‚úì | Passes | Requirement verified successfully |
| ‚úó | Fails | Requirement failed verification |
| ‚óã | Skipped | User chose to skip |
| ‚Äî | Pending | Not yet tested |

### Section Status
| Status | Meaning | Condition |
|--------|---------|-----------|
| `‚úì VERIFIED` | Section complete | All requirements have ‚úì |
| `‚óã IN PROGRESS` | Partially tested | Mix of ‚úì, ‚úó, ‚óã, ‚Äî |
| `‚Äî NOT STARTED` | Not begun | All requirements have ‚Äî |

### When to Create/Update
| Moment | Action |
|--------|--------|
| Start FASE 2 | Create file with all requirements as `‚Äî` (pending) |
| After each test | Update requirement result (‚úì, ‚úó, or ‚óã) |
| Issue found | Add to "Issues Found" table |
| Section complete | Update section status |
| End FASE 6 | Update summary, timestamp |

---

## Best Practices

### Language
Follow the Language Policy in CLAUDE.md.

### Notifications
- **Notify when Claude waits for user input AFTER a long-running phase**
- Notification moments:
  - FASE 5 start (after FASE 4 research agents): "Debug plan ready"
  - FASE 6 start (after FASE 5 debug execution): "Fixes applied"
- Use the shared script: `.claude/scripts/notify.ps1` with `-Title` and `-Message` parameters
- Never skip notifications - user may be away from screen during agent execution

### Issue Assessment Agents (FASE 2)
- **When issue found, spawn 3 assessment agents in parallel:**
  - **assess-user-impact**: User perspective - how does this affect end users?
  - **assess-technical**: Developer perspective - what's the technical root cause?
  - **assess-business**: Business perspective - what's the risk/priority?
- **Calculate weighted severity:** `(user√ó0.40) + (technical√ó0.30) + (business√ó0.30)`
- **Severity thresholds:** 75-100 = CRITICAL, 50-74 = IMPORTANT, 0-49 = SUGGESTION

### Fix Strategy Agents (FASE 5)
- **After Context7 research, spawn 3 fix strategy agents in parallel:**
  - **fix-minimal**: "Smallest change that works" - hotfix approach, minimal risk
  - **fix-thorough**: "Fix it right, fix it once" - complete root cause fix
  - **fix-defensive**: "Never let this happen again" - add safeguards and validation
- **User chooses strategy** based on situation (urgency, quality needs, prevention)
- Present comparison table with changes, risk, time, and best-for criteria

### Issue Assessment Guidelines
- **CRITICAL** (score 75-100): Errors on screen, app breaks, data loss - always stop, fix first
- **IMPORTANT** (score 50-74): Features don't work correctly, wrong output, bad UX - log, continue, fix after
- **SUGGESTION** (score 0-49): Styling issues, minor UX, alignment - log, optional fix

### Research Agent Spawning (FASE 4)
- Match issue type to specialized agent (error-handling, performance, security, quality, best-practices)
- Use best-practices-researcher with focused prompts for TypeScript, UI, or general issues
- Spawn multiple specialized researchers in parallel for efficiency
- Always wait for agent results before proceeding
- Debug-executor should be fully autonomous after fix plan approval

### Requirement-Based Testing
- **ALWAYS use requirement-based testing** - requirements from 01-intent.md are mandatory
- Load requirements from 01-intent.md before starting tests
- Test each REQ-ID individually, update passes status
- **Update 01-intent.md** after each test to persist requirement status
- **If no requirements found:** Stop and instruct user to run /1-plan first
- Completion requires ALL requirements to have `passes: true`

### Test Execution Flow
- Keep number-based responses (1/2/3) for speed
- Never ask unnecessary confirmations
- Log everything for traceability
- Link issues to REQ-IDs when in requirement-based mode

### Standardized Terminology
Use consistent capitalization for all status outputs:
- **Passes ‚úì** - requirement verified successfully
- **Fails ‚úó** - requirement failed verification
- **Skipped** - requirement not tested (user chose to skip)
- **[CRITICAL]** - severity prefix for blocking issues
- **[IMPORTANT]** - severity prefix for significant issues
- **[SUGGESTION]** - severity prefix for minor improvements

Never use: "PASSES", "FAILS", "passes ‚úì" (inconsistent caps)

### Error Handling
- Report clear errors with context
- Attempt automatic recovery when possible
- Never silently skip critical steps

## Scripts

The skill uses these scripts from the verify folder:

### run_tests.py
- Detects and executes test framework
- Returns structured test results
- Supports all major frameworks

### create_attempt_log.py
- Creates detailed attempt logs
- Tracks debug history
- Maintains attempt numbering

### generate_verify_test_plan.py
- Creates re-verification test plans
- Focuses on fixed issues
- Generates targeted test scenarios

### append_verify_history.py
- Adds verification history to context files
- Documents lessons learned
- Maintains knowledge for future

## Agents

The skill spawns these specialized agents:

### Specialized Research Agents

**error-handling-researcher**
- Focus: Errors, exceptions, crashes, try/catch patterns
- Context7 queries: Error boundaries, exception handling, graceful degradation
- Best for: CRITICAL issues involving app crashes or errors on screen

**performance-researcher**
- Focus: Slow performance, memory issues, N+1 queries
- Context7 queries: Caching, optimization, query efficiency
- Best for: Performance degradation issues

**security-researcher**
- Focus: Security vulnerabilities, auth issues, input validation
- Context7 queries: OWASP patterns, auth flows, sanitization
- Best for: Security-related CRITICAL or IMPORTANT issues

**quality-researcher**
- Focus: Code quality, design patterns, SOLID principles
- Context7 queries: Refactoring patterns, maintainability, clean code
- Best for: Code smell issues and architectural problems

**best-practices-researcher**
- Focus: Framework-specific patterns, conventions, idioms
- Context7 queries: Framework best practices, common patterns
- Best for: TypeScript type issues, UI/styling bugs, and general issues (use focused prompts)

### Assessment Agents (FASE 2)

**assess-user-impact** (40% weight)
- Evaluates issue from end-user perspective
- Scores based on: blocking, visibility, frequency, workarounds
- Focus: "How does this affect the user experience?"

**assess-technical** (30% weight)
- Evaluates issue from developer perspective
- Scores based on: root cause, blast radius, fix complexity, technical debt
- Focus: "What's the technical severity and fix effort?"

**assess-business** (30% weight)
- Evaluates issue from business perspective
- Scores based on: revenue impact, reputation risk, compliance, strategic priority
- Focus: "What's the business risk and priority?"

### Fix Strategy Agents (FASE 5)

**fix-minimal**
- Philosophy: "Smallest change that works"
- Creates hotfix plans with lowest risk
- Best for: urgent production fixes, time-critical situations

**fix-thorough**
- Philosophy: "Fix it right, fix it once"
- Addresses root cause completely, adds tests
- Best for: core functionality, recurring bugs, quality priority

**fix-defensive**
- Philosophy: "Never let this happen again"
- Adds safeguards, validation, error handling
- Best for: preventing similar issues, user-facing features

### Support Agents

**fix-synthesizer**
- Analyzes Context7 results from all specialized researchers
- Creates actionable fix strategies prioritized by severity
- Combines findings into coherent debug plan

**debug-executor**
- Executes approved debug plans autonomously
- Applies fixes systematically in priority order
- Reports results back with file:line references

## Error Handling

This section documents all error scenarios and their handling.

### FASE 0 Errors

**Test plan file not found:**
```
‚ùå TEST PLAN NOT FOUND

Searched: {path}
Available test plans: {list}

Try again with correct name or path:
```
‚Üí Re-prompt for input

**No test plans exist (empty features directory):**
```
‚ùå NO TEST PLANS FOUND

Run /2-code first to implement a feature.
```
‚Üí Exit skill

**Invalid user input:**
```
‚ö†Ô∏è INVALID INPUT

Expected: feature name, path, or "recent"
Try again:
```
‚Üí Re-prompt for input

### FASE 1 Errors

**Malformed test plan file:**
```
‚ùå INVALID TEST PLAN FORMAT

File: {path}
Problem: {missing sections / invalid format}

Regenerate with /2-code or fix manually.
```
‚Üí Exit skill

**Empty test plan:**
```
‚ùå EMPTY TEST PLAN

The file contains no tests.
Run /2-code again to generate tests.
```
‚Üí Exit skill

### FASE 2 Errors

**Invalid test response:**
> **Note:** With AskUserQuestion tool, invalid responses are prevented by the option-based UI.
> User selects from: Passes | Fails | Skip

‚Üí No re-prompt needed (options are enforced)

**Empty issue description:**
```
‚ö†Ô∏è DESCRIPTION REQUIRED

Describe the issue (or type "skip" to skip):
```
‚Üí Re-prompt for description or allow skip

### FASE 4 Errors

**Agent spawn failure:**
```
‚ö†Ô∏è AGENT NOT AVAILABLE

Agent: {agent-name}
Reason: {error}

Fallback: Use Context7 directly in main conversation.
```
‚Üí Continue with fallback approach

**Context7 timeout or no results:**
```
‚ö†Ô∏è CONTEXT7 RESEARCH TIMEOUT

No results within {timeout}s.
```

**Use AskUserQuestion tool:**
- header: "Timeout"
- question: "Context7 research timed out. How do you want to proceed?"
- options:
  - label: "Retry", description: "Try again"
  - label: "Skip", description: "Create debug plan without research"
  - label: "Exit", description: "Stop verification"
- multiSelect: false
‚Üí Handle user choice accordingly

**Low quality results (relevance < 50%):**
```
‚ö†Ô∏è LOW RESEARCH QUALITY

Relevance: {X}% (target: ‚â•75%)

Quality interpretation:
- 75-100%: High confidence solutions
- 50-74%: Partial solutions, may need adaptation
- <50%: Limited relevant results, manual input recommended

The found solutions may not be applicable.
```

**Use AskUserQuestion tool:**
- header: "Low Quality"
- question: "Research quality is low. How do you want to proceed?"
- options:
  - label: "Continue", description: "Use available results (lower success chance)"
  - label: "Manual input", description: "Provide your own solution approach"
  - label: "Skip research", description: "Create debug plan based on analysis only"
  - label: "Retry", description: "Try different search queries"
- multiSelect: false

**Handle low quality response:**
- If "Continue": Proceed to FASE 5 with `low_quality_flag = true`
- If "Manual input":
  **Use AskUserQuestion tool:**
  - header: "Jouw Oplossing"
  - question: "Beschrijf je voorgestelde oplossingsaanpak:"
  - options:
    - label: "Invoeren (Recommended)", description: "Typ je oplossing in het tekstveld"
    - label: "Annuleren", description: "Ga terug naar vorige opties"
    - label: "Uitleg", description: "Wat voor informatie is nuttig?"
  - multiSelect: false
  - If user provides text input ‚Üí Store as `manual_solution`:
    ```python
    manual_solution = {
        "provided": True,
        "approach": "{user's solution description}",
        "source": "user_input"
    }
    ```
    Proceed to FASE 5 with `manual_solution` - this will be presented as a 4th strategy option
  - If "Annuleren" ‚Üí Return to low quality options
  - If "Uitleg" ‚Üí Explain what information helps, then re-prompt

- If "Skip research": Proceed to FASE 5 without Context7 results, use sequential thinking analysis only
- If "Retry":
  **Use AskUserQuestion tool:**
  - header: "Nieuwe Zoektermen"
  - question: "Stel alternatieve zoektermen of focusgebieden voor:"
  - options:
    - label: "Invoeren (Recommended)", description: "Typ nieuwe zoektermen in het tekstveld"
    - label: "Automatisch", description: "Laat Claude alternatieve queries genereren"
    - label: "Annuleren", description: "Ga terug naar vorige opties"
    - label: "Uitleg", description: "Welke zoektermen werken goed?"
  - multiSelect: false
  - If user provides text input ‚Üí Re-run Context7 research with new queries
  - If "Automatisch" ‚Üí Generate alternative queries based on issue analysis, re-run research
  - If "Annuleren" ‚Üí Return to low quality options
  - If "Uitleg" ‚Üí Explain effective search strategies, then re-prompt

### FASE 5 Errors

**Debug executor failure:**
```
‚ö†Ô∏è DEBUG EXECUTION FAILED

Step: {step number}
Error: {error message}
```

**Use AskUserQuestion tool:**
- header: "Debug Fail"
- question: "Debug execution failed. How do you want to proceed?"
- options:
  - label: "Retry step", description: "Try this step again"
  - label: "Skip step", description: "Continue to next step"
  - label: "Stop", description: "Cancel debug execution"
- multiSelect: false
‚Üí Handle user choice accordingly

### FASE 6 Errors

**Script execution failure:**
```
‚ö†Ô∏è TEST SCRIPT FAILED

Script: {script name}
Error: {error}

Run tests handmatig of fix script error.
```
‚Üí Continue with manual testing

### General Errors

**User wants to exit at any point:**
At any prompt, user can type "exit" or "stop":
```
Verification stopped.

Current status saved in 03-verify.md.
Issues found: {count}
Tests completed: {X}/{Y}

Resume later with /3-verify {feature-name}
```
‚Üí Save state to 03-verify.md and exit

## Restrictions

This skill must NEVER:
- **Wait for user to ask for tests** - FASE 2 must start presenting requirements immediately after FASE 1
- Skip sequential thinking when issues are found in FASE 2
- Proceed without user confirmation for debug plan execution
- Skip issue logging when continuing tests
- Apply fixes without Context7 research
- Mark verification complete if requirements still fail
- **Mark feature complete if ANY requirement in 03-verify.md has result ‚úó or ‚Äî**
- **Skip loading requirements from 01-intent.md when available**
- **Skip reading existing files** (00-overview.md, 02-implementation.md, 04-refine-*.md, 05-refactor-*.md) when present in feature folder
- **Ignore previous implementation/refinement/refactor context** in verification
- **Skip spawning 3 assessment agents in FASE 2** (assess-user-impact, assess-technical, assess-business)
- **Skip spawning 3 fix strategy agents in FASE 5** (fix-minimal, fix-thorough, fix-defensive)
- **Apply fix strategy without user selection** from the 3 options

This skill must ALWAYS:
- Ask for test plan location in FASE 0
- **Immediately transition to FASE 2 after FASE 1** - present first requirement without waiting
- **Present requirements one by one** - show each test, wait for response, then immediately show next
- **Create 03-verify.md at start of FASE 2** if it doesn't exist (see "03-verify.md Format" section)
- **Load ALL files in feature folder** (not just 01-intent.md and 02-tests.md)
- **Load testable requirements from 01-intent.md in FASE 1**
- **Detect part/extend/change via sections in 01-intent.md** (not separate folders)
- **Parse part requirements from "## Part:" sections** when in part mode
- **Parse extend/change requirements from "## Extend/Change:" sections** when in extend/change mode
- **Use requirement-based testing when requirements are available**
- Use sequential thinking for issue assessment
- Log all test results (passed/failed/skipped) with REQ-ID links
- **Update 03-verify.md after each test** (update requirement result, summary counts, section status)
- Create debug plan before fixes
- Spawn agents for complex operations
- Update 02-tests.md with final results (append section for part/extend/change)
- Update 00-overview.md status (Verified ‚úì/‚úó) if the file exists
- Provide clear next steps
- **Check completion: all requirements must pass before declaring complete**
- **Spawn 3 assessment agents in parallel when issues found** (FASE 2)
- **Calculate weighted severity from 3 agent scores** (user√ó0.40 + technical√ó0.30 + business√ó0.30)
- **Spawn 3 fix strategy agents in parallel after Context7 research** (FASE 5)
- **Present fix strategy comparison table and wait for user selection**